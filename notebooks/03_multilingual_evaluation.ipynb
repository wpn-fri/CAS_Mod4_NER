{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft / Ideas: Multilingual Evaluation\n",
    "\n",
    "## Overview\n",
    "This notebook evaluates the fine-tuned XLM-RoBERTa model on multiple languages to test **cross-lingual transfer**.\n",
    "\n",
    "**Tests**\n",
    "1. **English test set** (in-domain, literary text, full schema)\n",
    "2. **French test set** (in-domain, literary text, full schema)\n",
    "3. **German literary text** (placeholder for manual annotations ?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Hugging Face libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Evaluation metrics\n",
    "from seqeval.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found trained model at: /storage/homefs/nw03x063/CAS_Mod4_NER/notebooks/../models/litbank-xlm-roberta\n"
     ]
    }
   ],
   "source": [
    "# Configure paths\n",
    "MODEL_PATH = Path(\"../models/litbank-xlm-roberta\")\n",
    "PROCESSED_DATA_PATH = Path(\"../data/processed\")\n",
    "RESULTS_PATH = Path(\"../results\")\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if model exists\n",
    "if not MODEL_PATH.exists():\n",
    "    print(f\"⚠️  Model not found at {MODEL_PATH}\")\n",
    "    print(\"Please run Notebook 2 (Model Training) first to train the model.\")\n",
    "else:\n",
    "    print(f\"✓ Found trained model at: {MODEL_PATH.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Fine-tuned Model\n",
    "\n",
    "We'll load the XLM-RoBERTa model trained on English LitBank data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '../models/litbank-xlm-roberta' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Model loaded successfully\n",
      "  Model: XLM-RoBERTa fine-tuned on LitBank\n",
      "  Parameters: 277,464,591\n",
      "  Entity types: 7\n",
      "\n",
      "Supported entity types:\n",
      "  - FAC\n",
      "  - GPE\n",
      "  - LOC\n",
      "  - ORG\n",
      "  - PER\n",
      "  - TIME\n",
      "  - VEH\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "print(\"Loading fine-tuned model...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Load label mapping\n",
    "with open(PROCESSED_DATA_PATH / \"label_mapping.json\", 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "label2id = label_mapping[\"label2id\"]\n",
    "id2label = {int(k): v for k, v in label_mapping[\"id2label\"].items()}\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully\")\n",
    "print(f\"  Model: XLM-RoBERTa fine-tuned on LitBank\")\n",
    "print(f\"  Parameters: {model.num_parameters():,}\")\n",
    "print(f\"  Entity types: {len([l for l in label2id if l.startswith('B-')])}\")\n",
    "print(f\"\\nSupported entity types:\")\n",
    "entity_types = sorted(set([l[2:] for l in label2id.keys() if l.startswith('B-')]))\n",
    "for entity_type in entity_types:\n",
    "    print(f\"  - {entity_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create NER Pipeline\n",
    "\n",
    "- Simplifies inference (handles tokenization, prediction, decoding)\n",
    "- Aggregates subword predictions into word-level entities\n",
    "- Provides confidence scores\n",
    "\n",
    "Example test passage from Charles Dickens' *Christmas Carol*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NER pipeline created\n",
      "  Device: GPU\n",
      "\n",
      "Test prediction:\n",
      "  Input: Although they had but that moment left the school behind them, they were now in the busy thoroughfares of a city, where shadowy passengers passed and re-passed; where shadowy carts and coaches battled for the way, and all the strife and tumult of a real city were. It was made plain enough, by the dressing of the shops, that here, too, it was Christmas-time again; but it was evening, and the streets were lighted up. The Ghost stopped at a certain warehouse door, and asked Scrooge if he knew it.\n",
      "\n",
      "  Detected entities:\n",
      "    - the school           → FAC    (confidence: 0.936)\n",
      "    - the busy thoroughfares of → FAC    (confidence: 0.825)\n",
      "    - a city               → GPE    (confidence: 0.868)\n",
      "    - shadowy passengers   → PER    (confidence: 0.972)\n",
      "    - shadowy carts        → VEH    (confidence: 0.635)\n",
      "    - coaches              → VEH    (confidence: 0.511)\n",
      "    - way                  → FAC    (confidence: 0.711)\n",
      "    - a real city          → GPE    (confidence: 0.852)\n",
      "    - the shops            → FAC    (confidence: 0.964)\n",
      "    - the streets          → FAC    (confidence: 0.867)\n",
      "    - The Ghost            → PER    (confidence: 0.907)\n",
      "    - a certain warehouse  → FAC    (confidence: 0.927)\n",
      "    - Scrooge              → PER    (confidence: 0.938)\n"
     ]
    }
   ],
   "source": [
    "# Create NER pipeline\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",  # Aggregate subwords into entities\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "print(\"✓ NER pipeline created\")\n",
    "print(f\"  Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Test on example sentence\n",
    "test_sentence = \"Although they had but that moment left the school behind them, they were now in the busy thoroughfares of a city, where shadowy passengers passed and re-passed; where shadowy carts and coaches battled for the way, and all the strife and tumult of a real city were. It was made plain enough, by the dressing of the shops, that here, too, it was Christmas-time again; but it was evening, and the streets were lighted up. The Ghost stopped at a certain warehouse door, and asked Scrooge if he knew it.\"\n",
    "print(f\"\\nTest prediction:\")\n",
    "print(f\"  Input: {test_sentence}\")\n",
    "predictions = ner_pipeline(test_sentence)\n",
    "print(f\"\\n  Detected entities:\")\n",
    "for entity in predictions:\n",
    "    print(f\"    - {entity['word']:20s} → {entity['entity_group']:6s} (confidence: {entity['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Section 1: English Test Set Evaluation\n",
    "\n",
    "- Domain: Literary texts (in-domain)\n",
    "- Language: English (training language)\n",
    "- Schema: Full 6 entity types (PER, LOC, GPE, ORG, FAC, VEH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English test set...\n",
      "✓ Loaded 693 English test examples\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(data_path: Path) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Load test data with tokens and labels.\n",
    "    \n",
    "    Returns:\n",
    "        all_tokens: List of token sequences\n",
    "        all_labels: List of label sequences (BIO tags)\n",
    "    \"\"\"\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    all_tokens = [example['tokens'] for example in test_data]\n",
    "    \n",
    "    # Convert label IDs back to strings\n",
    "    all_labels = []\n",
    "    for example in test_data:\n",
    "        labels = [id2label[label_id] for label_id in example['ner_tags']]\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    return all_tokens, all_labels\n",
    "\n",
    "\n",
    "# Load English test data\n",
    "print(\"Loading English test set...\")\n",
    "en_tokens, en_labels = load_test_data(PROCESSED_DATA_PATH / \"english_test.json\")\n",
    "print(f\"✓ Loaded {len(en_tokens)} English test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prediction functions defined\n"
     ]
    }
   ],
   "source": [
    "def predict_sequences(tokens_list: List[List[str]], pipeline) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Generate predictions for token sequences.\n",
    "    \n",
    "    WHY: We need to convert token lists back to text, run inference,\n",
    "    then align predictions with original tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens_list: List of token sequences\n",
    "        pipeline: Hugging Face NER pipeline\n",
    "        \n",
    "    Returns:\n",
    "        List of predicted label sequences (aligned with input tokens)\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    \n",
    "    for tokens in tqdm(tokens_list, desc=\"Predicting\"):\n",
    "        # Join tokens to text (approximate reconstruction)\n",
    "        text = \" \".join(tokens)\n",
    "        \n",
    "        # Get predictions from pipeline\n",
    "        entities = pipeline(text)\n",
    "        \n",
    "        # Align predictions with original tokens\n",
    "        predictions = align_predictions_with_tokens(tokens, entities, text)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def align_predictions_with_tokens(tokens: List[str], \n",
    "                                 entities: List[Dict], \n",
    "                                 text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Align entity predictions with original token positions.\n",
    "    \n",
    "    WHY: Pipeline returns character-based spans, we need token-level labels.\n",
    "    \"\"\"\n",
    "    # Initialize all as 'O' (outside entity)\n",
    "    labels = ['O'] * len(tokens)\n",
    "    \n",
    "    # Build character-to-token mapping\n",
    "    char_to_token = {}\n",
    "    char_pos = 0\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        token_len = len(token)\n",
    "        for i in range(token_len):\n",
    "            char_to_token[char_pos + i] = token_idx\n",
    "        char_pos += token_len + 1  # +1 for space\n",
    "    \n",
    "    # Map entities to tokens\n",
    "    for entity in entities:\n",
    "        start_char = entity['start']\n",
    "        end_char = entity['end']\n",
    "        entity_type = entity['entity_group']\n",
    "        \n",
    "        # Find which tokens this entity spans\n",
    "        start_token = char_to_token.get(start_char)\n",
    "        end_token = char_to_token.get(end_char - 1)\n",
    "        \n",
    "        if start_token is not None and end_token is not None:\n",
    "            # First token gets B- label\n",
    "            labels[start_token] = f\"B-{entity_type}\"\n",
    "            \n",
    "            # Remaining tokens get I- label\n",
    "            for token_idx in range(start_token + 1, end_token + 1):\n",
    "                labels[token_idx] = f\"I-{entity_type}\"\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "print(\"✓ Prediction functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for English test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bf034c147549aba2d70c5759b12581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/693 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENGLISH TEST SET RESULTS (In-Domain)\n",
      "============================================================\n",
      "  Precision: 0.6462\n",
      "  Recall:    0.6952\n",
      "  F1 Score:  0.6698\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FAC     0.4925    0.5078    0.5000       193\n",
      "         GPE     0.6000    0.8500    0.7034        60\n",
      "         LOC     0.5042    0.5505    0.5263       109\n",
      "         ORG     0.0833    0.1667    0.1111         6\n",
      "         PER     0.7125    0.7500    0.7308       856\n",
      "         VEH     0.5556    0.6250    0.5882        16\n",
      "\n",
      "   micro avg     0.6462    0.6952    0.6698      1240\n",
      "   macro avg     0.4913    0.5750    0.5267      1240\n",
      "weighted avg     0.6495    0.6952    0.6707      1240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for English test set\n",
    "print(\"Generating predictions for English test set...\")\n",
    "en_predictions = predict_sequences(en_tokens, ner_pipeline)\n",
    "\n",
    "# Compute metrics\n",
    "en_results = {\n",
    "    'precision': precision_score(en_labels, en_predictions),\n",
    "    'recall': recall_score(en_labels, en_predictions),\n",
    "    'f1': f1_score(en_labels, en_predictions)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENGLISH TEST SET RESULTS (In-Domain)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Precision: {en_results['precision']:.4f}\")\n",
    "print(f\"  Recall:    {en_results['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {en_results['f1']:.4f}\")\n",
    "\n",
    "# Detailed report\n",
    "print(\"\\n\" + classification_report(en_labels, en_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Section 2: Cross-Lingual Evaluation\n",
    "\n",
    "### 4.1 German Literary Text (Placeholder)\n",
    "\n",
    "**Setup:**\n",
    "- Domain: Literary texts (in-domain)\n",
    "- Language: German (zero-shot)\n",
    "- Schema: Full 6 entity types\n",
    "\n",
    "**Note:** This section includes placeholder data. You can add manually annotated German literary excerpts here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German examples: 2\n",
      "\n",
      "⚠️  Note: Currently using minimal placeholder data.\n",
      "   To fully test German, add more annotated examples above.\n"
     ]
    }
   ],
   "source": [
    "# German literary text examples (manually annotated - TO BE ADDED)\n",
    "german_examples = [\n",
    "    {\n",
    "        \"text\": \"Der junge Werther reiste von Frankfurt nach Wahlheim.\",\n",
    "        \"tokens\": [\"Der\", \"junge\", \"Werther\", \"reiste\", \"von\", \"Frankfurt\", \"nach\", \"Wahlheim\", \".\"],\n",
    "        \"labels\": [\"O\", \"O\", \"B-PER\", \"O\", \"O\", \"B-LOC\", \"O\", \"B-LOC\", \"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"In Berlin traf Faust den Gelehrten Wagner.\",\n",
    "        \"tokens\": [\"In\", \"Berlin\", \"traf\", \"Faust\", \"den\", \"Gelehrten\", \"Wagner\", \".\"],\n",
    "        \"labels\": [\"O\", \"B-LOC\", \"O\", \"B-PER\", \"O\", \"O\", \"B-PER\", \"O\"]\n",
    "    },\n",
    "    # ADD MORE EXAMPLES HERE\n",
    "]\n",
    "\n",
    "print(f\"German examples: {len(german_examples)}\")\n",
    "print(\"\\n⚠️  Note: Currently using minimal placeholder data.\")\n",
    "print(\"   To fully test German, add more annotated examples above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for German literary texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2cd0a0e6cb4f0885ca01e17c6562ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GERMAN LITERARY TEXT RESULTS (Zero-Shot)\n",
      "============================================================\n",
      "  Examples:  2\n",
      "  Precision: 0.1667\n",
      "  Recall:    0.1667\n",
      "  F1 Score:  0.1667\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GPE     0.0000    0.0000    0.0000         0\n",
      "         LOC     0.0000    0.0000    0.0000         3\n",
      "         PER     0.3333    0.3333    0.3333         3\n",
      "\n",
      "   micro avg     0.1667    0.1667    0.1667         6\n",
      "   macro avg     0.1111    0.1111    0.1111         6\n",
      "weighted avg     0.1667    0.1667    0.1667         6\n",
      "\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "  Example 1: Der junge Werther reiste von Frankfurt nach Wahlheim.\n",
      "    Der             | True: O        | Pred: B-PER    ✗\n",
      "    junge           | True: O        | Pred: I-PER    ✗\n",
      "    Werther         | True: B-PER    | Pred: I-PER    ✗\n",
      "    Frankfurt       | True: B-LOC    | Pred: B-GPE    ✗\n",
      "    Wahlheim        | True: B-LOC    | Pred: B-GPE    ✗\n",
      "\n",
      "  Example 2: In Berlin traf Faust den Gelehrten Wagner.\n",
      "    Berlin          | True: B-LOC    | Pred: B-GPE    ✗\n",
      "    Faust           | True: B-PER    | Pred: B-PER    ✓\n",
      "    den             | True: O        | Pred: B-PER    ✗\n",
      "    Gelehrten       | True: O        | Pred: I-PER    ✗\n",
      "    Wagner          | True: B-PER    | Pred: I-PER    ✗\n"
     ]
    }
   ],
   "source": [
    "if german_examples:\n",
    "    # Extract tokens and labels\n",
    "    de_tokens = [ex['tokens'] for ex in german_examples]\n",
    "    de_labels = [ex['labels'] for ex in german_examples]\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions for German literary texts...\")\n",
    "    de_predictions = predict_sequences(de_tokens, ner_pipeline)\n",
    "    \n",
    "    # Compute metrics\n",
    "    de_results = {\n",
    "        'precision': precision_score(de_labels, de_predictions),\n",
    "        'recall': recall_score(de_labels, de_predictions),\n",
    "        'f1': f1_score(de_labels, de_predictions)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GERMAN LITERARY TEXT RESULTS (Zero-Shot)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Examples:  {len(german_examples)}\")\n",
    "    print(f\"  Precision: {de_results['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {de_results['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {de_results['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + classification_report(de_labels, de_predictions, digits=4))\n",
    "    \n",
    "    # Show example predictions\n",
    "    print(\"\\nExample predictions:\")\n",
    "    for i, example in enumerate(german_examples[:2]):\n",
    "        print(f\"\\n  Example {i+1}: {example['text']}\")\n",
    "        for token, true_label, pred_label in zip(de_tokens[i], de_labels[i], de_predictions[i]):\n",
    "            if true_label != 'O' or pred_label != 'O':\n",
    "                match = \"✓\" if true_label == pred_label else \"✗\"\n",
    "                print(f\"    {token:15s} | True: {true_label:8s} | Pred: {pred_label:8s} {match}\")\n",
    "else:\n",
    "    print(\"⚠️  No German examples provided. Skipping German evaluation.\")\n",
    "    de_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 French News Dataset (Out-of-Domain)\n",
    "\n",
    "**Setup:**\n",
    "- Domain: News articles (out-of-domain from literary training data)\n",
    "- Language: French (zero-shot)\n",
    "- Schema: **Overlapping tags only** (PER, ORG, LOC)\n",
    "\n",
    "**Why different schema?**\n",
    "- News datasets typically don't annotate FAC/VEH\n",
    "- GPE often merged with LOC in news\n",
    "- Tests domain adaptation: literary → news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French news examples (overlapping schema: PER, ORG, LOC)\n",
    "french_examples = [\n",
    "    {\n",
    "        \"text\": \"Le président Emmanuel Macron a visité Paris hier.\",\n",
    "        \"tokens\": [\"Le\", \"président\", \"Emmanuel\", \"Macron\", \"a\", \"visité\", \"Paris\", \"hier\", \".\"],\n",
    "        \"labels\": [\"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"L'ONU a organisé une conférence à Genève.\",\n",
    "        \"tokens\": [\"L'\", \"ONU\", \"a\", \"organisé\", \"une\", \"conférence\", \"à\", \"Genève\", \".\"],\n",
    "        \"labels\": [\"O\", \"B-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"La ministre Sophie Dupont représente la France.\",\n",
    "        \"tokens\": [\"La\", \"ministre\", \"Sophie\", \"Dupont\", \"représente\", \"la\", \"France\", \".\"],\n",
    "        \"labels\": [\"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"B-LOC\", \"O\"]\n",
    "    },\n",
    "    # ADD MORE FRENCH NEWS EXAMPLES HERE\n",
    "]\n",
    "\n",
    "print(f\"French news examples: {len(french_examples)}\")\n",
    "print(\"\\n⚠️  Note: Currently using minimal placeholder data.\")\n",
    "print(\"   To fully test French, add more annotated news examples above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_overlapping_entities(labels: List[List[str]], \n",
    "                               predictions: List[List[str]], \n",
    "                               allowed_types: List[str]) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Filter labels to only include overlapping entity types.\n",
    "    \n",
    "    WHY: French news uses different schema than LitBank.\n",
    "    We only evaluate on entity types present in both.\n",
    "    \n",
    "    Args:\n",
    "        labels: True labels\n",
    "        predictions: Predicted labels\n",
    "        allowed_types: Entity types to keep (e.g., [\"PER\", \"ORG\", \"LOC\"])\n",
    "        \n",
    "    Returns:\n",
    "        Filtered labels and predictions\n",
    "    \"\"\"\n",
    "    filtered_labels = []\n",
    "    filtered_predictions = []\n",
    "    \n",
    "    for label_seq, pred_seq in zip(labels, predictions):\n",
    "        filtered_label = []\n",
    "        filtered_pred = []\n",
    "        \n",
    "        for label, pred in zip(label_seq, pred_seq):\n",
    "            # Extract entity type (remove B-/I- prefix)\n",
    "            label_type = label.split('-')[1] if '-' in label else None\n",
    "            pred_type = pred.split('-')[1] if '-' in pred else None\n",
    "            \n",
    "            # Keep only allowed types\n",
    "            if label_type in allowed_types or label == 'O':\n",
    "                filtered_label.append(label)\n",
    "            else:\n",
    "                filtered_label.append('O')  # Convert non-overlapping to O\n",
    "            \n",
    "            if pred_type in allowed_types or pred == 'O':\n",
    "                filtered_pred.append(pred)\n",
    "            else:\n",
    "                filtered_pred.append('O')\n",
    "        \n",
    "        filtered_labels.append(filtered_label)\n",
    "        filtered_predictions.append(filtered_pred)\n",
    "    \n",
    "    return filtered_labels, filtered_predictions\n",
    "\n",
    "\n",
    "print(\"✓ Filtering function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if french_examples:\n",
    "    # Extract tokens and labels\n",
    "    fr_tokens = [ex['tokens'] for ex in french_examples]\n",
    "    fr_labels = [ex['labels'] for ex in french_examples]\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions for French news texts...\")\n",
    "    fr_predictions_raw = predict_sequences(fr_tokens, ner_pipeline)\n",
    "    \n",
    "    # Filter to overlapping schema (PER, ORG, LOC only)\n",
    "    OVERLAPPING_TYPES = ['PER', 'ORG', 'LOC']\n",
    "    fr_labels_filtered, fr_predictions = filter_overlapping_entities(\n",
    "        fr_labels, \n",
    "        fr_predictions_raw, \n",
    "        OVERLAPPING_TYPES\n",
    "    )\n",
    "    \n",
    "    # Compute metrics\n",
    "    fr_results = {\n",
    "        'precision': precision_score(fr_labels_filtered, fr_predictions),\n",
    "        'recall': recall_score(fr_labels_filtered, fr_predictions),\n",
    "        'f1': f1_score(fr_labels_filtered, fr_predictions)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FRENCH NEWS RESULTS (Zero-Shot, Out-of-Domain)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Examples:  {len(french_examples)}\")\n",
    "    print(f\"  Schema:    Overlapping tags only (PER, ORG, LOC)\")\n",
    "    print(f\"  Precision: {fr_results['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {fr_results['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {fr_results['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + classification_report(fr_labels_filtered, fr_predictions, digits=4))\n",
    "    \n",
    "    # Show example predictions\n",
    "    print(\"\\nExample predictions:\")\n",
    "    for i, example in enumerate(french_examples[:2]):\n",
    "        print(f\"\\n  Example {i+1}: {example['text']}\")\n",
    "        for token, true_label, pred_label in zip(fr_tokens[i], fr_labels_filtered[i], fr_predictions[i]):\n",
    "            if true_label != 'O' or pred_label != 'O':\n",
    "                match = \"✓\" if true_label == pred_label else \"✗\"\n",
    "                print(f\"    {token:15s} | True: {true_label:8s} | Pred: {pred_label:8s} {match}\")\n",
    "else:\n",
    "    print(\"⚠️  No French examples provided. Skipping French evaluation.\")\n",
    "    fr_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Lingual Comparison Table\n",
    "\n",
    "Summary of performance across all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = [\n",
    "    {\n",
    "        'Language': 'English',\n",
    "        'Domain': 'Literary (in-domain)',\n",
    "        'Schema': 'Full (6 types)',\n",
    "        'Examples': len(en_tokens),\n",
    "        'Precision': en_results['precision'],\n",
    "        'Recall': en_results['recall'],\n",
    "        'F1': en_results['f1']\n",
    "    }\n",
    "]\n",
    "\n",
    "if de_results:\n",
    "    comparison_data.append({\n",
    "        'Language': 'German',\n",
    "        'Domain': 'Literary (in-domain)',\n",
    "        'Schema': 'Full (6 types)',\n",
    "        'Examples': len(german_examples),\n",
    "        'Precision': de_results['precision'],\n",
    "        'Recall': de_results['recall'],\n",
    "        'F1': de_results['f1']\n",
    "    })\n",
    "\n",
    "if fr_results:\n",
    "    comparison_data.append({\n",
    "        'Language': 'French',\n",
    "        'Domain': 'News (out-of-domain)',\n",
    "        'Schema': 'Overlapping (PER/ORG/LOC)',\n",
    "        'Examples': len(french_examples),\n",
    "        'Precision': fr_results['precision'],\n",
    "        'Recall': fr_results['recall'],\n",
    "        'F1': fr_results['f1']\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTILINGUAL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Save to file\n",
    "df_comparison.to_csv(RESULTS_PATH / \"multilingual_comparison.csv\", index=False)\n",
    "print(f\"\\n✓ Saved comparison table to {RESULTS_PATH / 'multilingual_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "languages = df_comparison['Language'].tolist()\n",
    "x = np.arange(len(languages))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, df_comparison['Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar(x, df_comparison['Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width, df_comparison['F1'], width, label='F1 Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Language', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Cross-Lingual NER Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{lang}\\n({domain})\" for lang, domain in zip(df_comparison['Language'], df_comparison['Domain'])], fontsize=10)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / \"multilingual_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved visualization to {RESULTS_PATH / 'multilingual_comparison.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Attention Weight Visualization\n",
    "\n",
    "**Why visualize attention?**\n",
    "- Shows which tokens the model focuses on when making predictions\n",
    "- Reveals if model attends to relevant context (nearby names, titles, etc.)\n",
    "- Helps understand cross-lingual transfer mechanisms\n",
    "\n",
    "**What to look for:**\n",
    "- Strong attention to capitalized words (likely entities)\n",
    "- Attention to title words (\"Dr.\", \"President\", etc.)\n",
    "- Context dependencies (city names after \"in\", \"from\", etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(text: str, model, tokenizer, layer: int = -1, head: int = 0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a given text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        model: Transformer model\n",
    "        tokenizer: Tokenizer\n",
    "        layer: Which layer to visualize (-1 = last layer)\n",
    "        head: Which attention head to visualize\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get model outputs with attention weights\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Extract attention weights\n",
    "    # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "    attention = outputs.attentions[layer][0, head].cpu().numpy()\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Plot attention heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    im = ax.imshow(attention, cmap='viridis', aspect='auto')\n",
    "    \n",
    "    # Set ticks\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=90, fontsize=9)\n",
    "    ax.set_yticklabels(tokens, fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Key Tokens (attending to)', fontsize=11)\n",
    "    ax.set_ylabel('Query Tokens (attending from)', fontsize=11)\n",
    "    ax.set_title(f'Attention Weights (Layer {layer}, Head {head})\\n\"{text}\"', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Attention Weight', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"✓ Attention visualization function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for example sentences\n",
    "example_sentences = [\n",
    "    (\"English\", \"Dr. Frankenstein traveled from Geneva to the Arctic.\"),\n",
    "]\n",
    "\n",
    "if de_results:\n",
    "    example_sentences.append((\"German\", \"Der junge Werther reiste von Frankfurt nach Wahlheim.\"))\n",
    "\n",
    "if fr_results:\n",
    "    example_sentences.append((\"French\", \"Le président Emmanuel Macron a visité Paris hier.\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ATTENTION WEIGHT VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for lang, text in example_sentences:\n",
    "    print(f\"\\nVisualizing: {lang} - \\\"{text}\\\"\")\n",
    "    fig = visualize_attention(text, model, tokenizer, layer=-1, head=0)\n",
    "    \n",
    "    # Save figure\n",
    "    filename = f\"attention_{lang.lower()}.png\"\n",
    "    fig.savefig(RESULTS_PATH / filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved to {RESULTS_PATH / filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n✓ All attention visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Qualitative Analysis\n",
    "\n",
    "Let's analyze some interesting predictions to understand model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(tokens: List[List[str]], \n",
    "                   labels: List[List[str]], \n",
    "                   predictions: List[List[str]], \n",
    "                   language: str):\n",
    "    \"\"\"\n",
    "    Identify and display common error patterns.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ERROR ANALYSIS: {language.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Track error types\n",
    "    false_positives = defaultdict(int)  # Predicted entity, but was O\n",
    "    false_negatives = defaultdict(int)  # Missed entity (predicted O, but was entity)\n",
    "    misclassifications = defaultdict(lambda: defaultdict(int))  # Wrong entity type\n",
    "    \n",
    "    for token_seq, label_seq, pred_seq in zip(tokens, labels, predictions):\n",
    "        for token, label, pred in zip(token_seq, label_seq, pred_seq):\n",
    "            if label != pred:\n",
    "                if label == 'O' and pred != 'O':\n",
    "                    # False positive\n",
    "                    pred_type = pred.split('-')[1] if '-' in pred else pred\n",
    "                    false_positives[pred_type] += 1\n",
    "                \n",
    "                elif label != 'O' and pred == 'O':\n",
    "                    # False negative\n",
    "                    label_type = label.split('-')[1] if '-' in label else label\n",
    "                    false_negatives[label_type] += 1\n",
    "                \n",
    "                elif label != 'O' and pred != 'O':\n",
    "                    # Misclassification\n",
    "                    label_type = label.split('-')[1] if '-' in label else label\n",
    "                    pred_type = pred.split('-')[1] if '-' in pred else pred\n",
    "                    misclassifications[label_type][pred_type] += 1\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nFalse Positives (predicted entity, but was O):\")\n",
    "    for entity_type, count in sorted(false_positives.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {entity_type:6s}: {count:3d} tokens incorrectly marked\")\n",
    "    \n",
    "    print(f\"\\nFalse Negatives (missed entities):\")\n",
    "    for entity_type, count in sorted(false_negatives.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {entity_type:6s}: {count:3d} tokens missed\")\n",
    "    \n",
    "    print(f\"\\nMisclassifications (wrong entity type):\")\n",
    "    for true_type, pred_counts in sorted(misclassifications.items()):\n",
    "        for pred_type, count in sorted(pred_counts.items(), key=lambda x: -x[1]):\n",
    "            print(f\"  {true_type:6s} → {pred_type:6s}: {count:3d} times\")\n",
    "\n",
    "\n",
    "# Analyze errors for each language\n",
    "analyze_errors(en_tokens[:100], en_labels[:100], en_predictions[:100], \"English\")  # Limit to first 100 for readability\n",
    "\n",
    "if de_results:\n",
    "    analyze_errors(de_tokens, de_labels, de_predictions, \"German\")\n",
    "\n",
    "if fr_results:\n",
    "    analyze_errors(fr_tokens, fr_labels_filtered, fr_predictions, \"French\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_kernel",
   "language": "python",
   "name": "conda_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
