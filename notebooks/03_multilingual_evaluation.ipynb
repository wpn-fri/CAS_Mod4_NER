{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft / Ideas: Multilingual Evaluation\n",
    "\n",
    "## Overview\n",
    "This notebook evaluates the fine-tuned XLM-RoBERTa model on multiple languages to test **cross-lingual transfer**.\n",
    "\n",
    "**Tests**\n",
    "1. **English test set** (in-domain, literary text, full schema)\n",
    "2. **French test set** (in-domain, literary text, full schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Hugging Face libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found trained model at: /storage/homefs/nw03x063/CAS_Mod4_NER/notebooks/../models/litbank-xlm-roberta\n"
     ]
    }
   ],
   "source": [
    "# Configure paths\n",
    "MODEL_PATH = Path(\"../models/litbank-xlm-roberta\")\n",
    "PROCESSED_DATA_PATH = Path(\"../data/processed\")\n",
    "RESULTS_PATH = Path(\"../results\")\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if model exists\n",
    "if not MODEL_PATH.exists():\n",
    "    print(f\"⚠️  Model not found at {MODEL_PATH}\")\n",
    "    print(\"Please run Notebook 2 (Model Training) first to train the model.\")\n",
    "else:\n",
    "    print(f\"✓ Found trained model at: {MODEL_PATH.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Fine-tuned Model\n",
    "\n",
    "We'll load the XLM-RoBERTa model trained on English LitBank data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '../models/litbank-xlm-roberta' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Model loaded successfully\n",
      "  Model: XLM-RoBERTa fine-tuned on LitBank\n",
      "  Parameters: 277,464,591\n",
      "  Entity types: 7\n",
      "\n",
      "Supported entity types:\n",
      "  - FAC\n",
      "  - GPE\n",
      "  - LOC\n",
      "  - ORG\n",
      "  - PER\n",
      "  - TIME\n",
      "  - VEH\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "print(\"Loading fine-tuned model...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Load label mapping\n",
    "with open(PROCESSED_DATA_PATH / \"label_mapping.json\", 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "label2id = label_mapping[\"label2id\"]\n",
    "id2label = {int(k): v for k, v in label_mapping[\"id2label\"].items()}\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully\")\n",
    "print(f\"  Model: XLM-RoBERTa fine-tuned on LitBank\")\n",
    "print(f\"  Parameters: {model.num_parameters():,}\")\n",
    "print(f\"  Entity types: {len([l for l in label2id if l.startswith('B-')])}\")\n",
    "print(f\"\\nSupported entity types:\")\n",
    "entity_types = sorted(set([l[2:] for l in label2id.keys() if l.startswith('B-')]))\n",
    "for entity_type in entity_types:\n",
    "    print(f\"  - {entity_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create NER Pipeline\n",
    "\n",
    "- Simplifies inference (handles tokenization, prediction, decoding)\n",
    "- Aggregates subword predictions into word-level entities\n",
    "- Provides confidence scores\n",
    "\n",
    "Example test passage from Charles Dickens' *Christmas Carol*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NER pipeline created\n",
      "  Device: GPU\n",
      "\n",
      "Test prediction:\n",
      "  Input: Although they had but that moment left the school behind them, they were now in the busy thoroughfares of a city, where shadowy passengers passed and re-passed; where shadowy carts and coaches battled for the way, and all the strife and tumult of a real city were. It was made plain enough, by the dressing of the shops, that here, too, it was Christmas-time again; but it was evening, and the streets were lighted up. The Ghost stopped at a certain warehouse door, and asked Scrooge if he knew it.\n",
      "\n",
      "  Detected entities:\n",
      "    - the school           → FAC    (confidence: 0.936)\n",
      "    - the busy thoroughfares of → FAC    (confidence: 0.825)\n",
      "    - a city               → GPE    (confidence: 0.868)\n",
      "    - shadowy passengers   → PER    (confidence: 0.972)\n",
      "    - shadowy carts        → VEH    (confidence: 0.635)\n",
      "    - coaches              → VEH    (confidence: 0.511)\n",
      "    - way                  → FAC    (confidence: 0.711)\n",
      "    - a real city          → GPE    (confidence: 0.852)\n",
      "    - the shops            → FAC    (confidence: 0.964)\n",
      "    - the streets          → FAC    (confidence: 0.867)\n",
      "    - The Ghost            → PER    (confidence: 0.907)\n",
      "    - a certain warehouse  → FAC    (confidence: 0.927)\n",
      "    - Scrooge              → PER    (confidence: 0.938)\n"
     ]
    }
   ],
   "source": [
    "# Create NER pipeline\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",  # Aggregate subwords into entities\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "print(\"✓ NER pipeline created\")\n",
    "print(f\"  Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Test on example sentence\n",
    "test_sentence = \"Although they had but that moment left the school behind them, they were now in the busy thoroughfares of a city, where shadowy passengers passed and re-passed; where shadowy carts and coaches battled for the way, and all the strife and tumult of a real city were. It was made plain enough, by the dressing of the shops, that here, too, it was Christmas-time again; but it was evening, and the streets were lighted up. The Ghost stopped at a certain warehouse door, and asked Scrooge if he knew it.\"\n",
    "print(f\"\\nTest prediction:\")\n",
    "print(f\"  Input: {test_sentence}\")\n",
    "predictions = ner_pipeline(test_sentence)\n",
    "print(f\"\\n  Detected entities:\")\n",
    "for entity in predictions:\n",
    "    print(f\"    - {entity['word']:20s} → {entity['entity_group']:6s} (confidence: {entity['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color-coded NER visualization\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def display_ner_results(text: str, entities: List[Dict], show_confidence: bool = True):\n",
    "    \"\"\"\n",
    "    Display NER results with color-coded entity tags.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        entities: List of entity predictions from the pipeline\n",
    "        show_confidence: Whether to show confidence scores\n",
    "    \"\"\"\n",
    "    # Define colors for each entity type\n",
    "    entity_colors = {\n",
    "        'PER': '#ffa07a',  # Light salmon\n",
    "        'ORG': '#87ceeb',  # Sky blue\n",
    "        'LOC': '#98fb98',  # Pale green\n",
    "        'GPE': '#dda0dd',  # Plum\n",
    "        'FAC': '#f0e68c',  # Khaki\n",
    "        'VEH': '#ffb6c1',  # Light pink\n",
    "    }\n",
    "    \n",
    "    # Sort entities by their start position\n",
    "    sorted_entities = sorted(entities, key=lambda x: x['start'])\n",
    "    \n",
    "    # Build HTML with highlighted entities\n",
    "    html_parts = []\n",
    "    last_end = 0\n",
    "    \n",
    "    for entity in sorted_entities:\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        entity_text = entity['word']\n",
    "        entity_type = entity['entity_group']\n",
    "        confidence = entity['score']\n",
    "        \n",
    "        # Add text before entity\n",
    "        if start > last_end:\n",
    "            html_parts.append(text[last_end:start])\n",
    "        \n",
    "        # Add highlighted entity\n",
    "        color = entity_colors.get(entity_type, '#d3d3d3')\n",
    "        tooltip = f\"{entity_type}\"\n",
    "        if show_confidence:\n",
    "            tooltip += f\" ({confidence:.2f})\"\n",
    "        \n",
    "        html_parts.append(\n",
    "            f'<mark style=\"background-color: {color}; padding: 2px 4px; '\n",
    "            f'margin: 0 2px; border-radius: 3px; font-weight: bold;\" '\n",
    "            f'title=\"{tooltip}\">'\n",
    "            f'{entity_text} '\n",
    "            f'<sup style=\"font-size: 0.7em; color: #555;\">[{entity_type}]</sup>'\n",
    "            f'</mark>'\n",
    "        )\n",
    "        \n",
    "        last_end = end\n",
    "    \n",
    "    # Add remaining text\n",
    "    if last_end < len(text):\n",
    "        html_parts.append(text[last_end:])\n",
    "    \n",
    "    # Create legend\n",
    "    legend_html = '<div style=\"margin-top: 20px; padding: 10px; background-color: #f5f5f5; border-radius: 5px;\">'\n",
    "    legend_html += '<strong>Entity Types:</strong><br>'\n",
    "    for entity_type, color in entity_colors.items():\n",
    "        legend_html += f'<span style=\"background-color: {color}; padding: 2px 8px; margin: 2px; border-radius: 3px; display: inline-block;\">{entity_type}</span> '\n",
    "    legend_html += '</div>'\n",
    "    \n",
    "    # Combine everything\n",
    "    full_html = f'<div style=\"line-height: 2.0; font-size: 14px;\">{\"\".join(html_parts)}</div>{legend_html}'\n",
    "    \n",
    "    display(HTML(full_html))\n",
    "\n",
    "# Display the test sentence with color-coded entities\n",
    "print(\"NER Results:\")\n",
    "display_ner_results(test_sentence, predictions, show_confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French Text Example\n",
    "\n",
    "Test the model's performance with custom French literary text (passage from Gui de Maupassant's *Pierre et Jean*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French test passage - replace with your own text\n",
    "french_text = \"Les cabines roulantes, attelées d'un cheval, remontaient aussi; et sur les planches de la promenade, qui borde la plage d'un bout à l'autre, c'était maintenant une coulée continue, épaisse et lente, de foule élégante, formant deux courants contraires qui se coudoyaient et se mêlaient. Pierre, nerveux, exaspéré par ce frôlement, s'enfuit, s'enfonça dans la ville et s'arrêta pour déjeuner chez un simple marchand de vins, à l'entrée des champs.\"\n",
    "\n",
    "# Get predictions\n",
    "french_predictions = ner_pipeline(french_text)\n",
    "\n",
    "print(\"Detected entities:\")\n",
    "for entity in french_predictions:\n",
    "    print(f\"  - {entity['word']:25s} → {entity['entity_group']:6s} (confidence: {entity['score']:.3f})\")\n",
    "\n",
    "# Display with color-coded tags\n",
    "print(\"\\nFrench NER Results:\")\n",
    "display_ner_results(french_text, french_predictions, show_confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Weight Visualization\n",
    "\n",
    "**Why visualize attention?**\n",
    "- Shows which tokens the model focuses on when making predictions\n",
    "- Reveals if model attends to relevant context (nearby names, titles, etc.)\n",
    "- Helps understand cross-lingual transfer mechanisms\n",
    "\n",
    "**What to look for:**\n",
    "- Strong attention to capitalized words (likely entities)\n",
    "- Attention to title words (\"Dr.\", \"President\", etc.)\n",
    "- Context dependencies (city names after \"in\", \"from\", etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(text: str, model, tokenizer, layer: int = -1, head: int = 0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a given text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        model: Transformer model\n",
    "        tokenizer: Tokenizer\n",
    "        layer: Which layer to visualize (-1 = last layer)\n",
    "        head: Which attention head to visualize\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get model outputs with attention weights\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Extract attention weights\n",
    "    # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "    attention = outputs.attentions[layer][0, head].cpu().numpy()\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Plot attention heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    im = ax.imshow(attention, cmap='viridis', aspect='auto')\n",
    "    \n",
    "    # Set ticks\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=90, fontsize=9)\n",
    "    ax.set_yticklabels(tokens, fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Key Tokens (attending to)', fontsize=11)\n",
    "    ax.set_ylabel('Query Tokens (attending from)', fontsize=11)\n",
    "    ax.set_title(f'Attention Weights (Layer {layer}, Head {head})\\n\"{text}\"', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Attention Weight', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"✓ Attention visualization function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize attention for the example sentences used above\nexample_sentences = [\n    (\"English\", test_sentence),\n    (\"French\", french_text),\n]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ATTENTION WEIGHT VISUALIZATIONS\")\nprint(\"=\"*60)\n\nfor lang, text in example_sentences:\n    print(f\"\\nVisualizing: {lang}\")\n    print(f\"Text: {text[:100]}...\")  # Show first 100 chars\n    \n    fig = visualize_attention(text, model, tokenizer, layer=-1, head=0)\n    \n    # Save figure\n    filename = f\"attention_{lang.lower()}.png\"\n    fig.savefig(RESULTS_PATH / filename, dpi=300, bbox_inches='tight')\n    print(f\"✓ Saved to {RESULTS_PATH / filename}\")\n    \n    plt.show()\n    plt.close()\n\nprint(\"\\n✓ All attention visualizations complete\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_kernel",
   "language": "python",
   "name": "conda_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}