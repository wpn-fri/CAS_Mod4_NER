{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Fine-tuning XLM-RoBERTa for NER\n",
    "\n",
    "## Overview\n",
    "This notebook fine-tunes a multilingual transformer model on the LitBank dataset.\n",
    "\n",
    "**What we'll do:**\n",
    "1. Load preprocessed data from Notebook 1\n",
    "2. Configure XLM-RoBERTa for token classification\n",
    "3. Handle subword tokenization (critical for transformers!)\n",
    "4. Train using Hugging Face Trainer\n",
    "5. Evaluate on English test set\n",
    "6. Visualize results (F1 scores, confusion matrix, training curves)\n",
    "\n",
    "**Why XLM-RoBERTa?**\n",
    "- Multilingual: Pre-trained on 100 languages\n",
    "- State-of-the-art performance on sequence labeling\n",
    "- Enables cross-lingual transfer (tested in Notebook 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Evaluation metrics\n",
    "from seqeval.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported\")\n",
    "print(\"\\nNote: First run will download XLM-RoBERTa (~1GB). This is normal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "PROCESSED_DATA_PATH = Path(\"../data/processed\")\n",
    "MODEL_OUTPUT_PATH = Path(\"../models\")\n",
    "RESULTS_PATH = Path(\"../results\")\n",
    "\n",
    "MODEL_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"xlm-roberta-base\"  # Multilingual model\n",
    "MAX_LENGTH = 512  # Maximum sequence length (tokens)\n",
    "BATCH_SIZE = 16   # Adjust based on your GPU memory (8/16/32)\n",
    "LEARNING_RATE = 2e-5  # Standard for fine-tuning transformers\n",
    "NUM_EPOCHS = 3    # Typically 3-5 epochs for NER\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Max length: {MAX_LENGTH} tokens\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "We'll load the JSON files created in Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(data_path: Path) -> Tuple[Dict, Dataset, Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Load preprocessed JSON data and convert to Hugging Face Dataset format.\n",
    "    \n",
    "    WHY: Hugging Face's Dataset class provides efficient batching and caching.\n",
    "    \n",
    "    Returns:\n",
    "        label_mapping: {\"label2id\": {...}, \"id2label\": {...}}\n",
    "        train_dataset: Training examples\n",
    "        dev_dataset: Validation examples\n",
    "        test_dataset: Test examples\n",
    "    \"\"\"\n",
    "    # Load label mapping\n",
    "    with open(data_path / \"label_mapping.json\", 'r', encoding='utf-8') as f:\n",
    "        label_mapping = json.load(f)\n",
    "    \n",
    "    # Load datasets\n",
    "    with open(data_path / \"train_data.json\", 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    with open(data_path / \"dev_data.json\", 'r', encoding='utf-8') as f:\n",
    "        dev_data = json.load(f)\n",
    "    \n",
    "    with open(data_path / \"test_data.json\", 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    # Convert to Hugging Face Dataset format\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    dev_dataset = Dataset.from_list(dev_data)\n",
    "    test_dataset = Dataset.from_list(test_data)\n",
    "    \n",
    "    print(f\"✓ Loaded preprocessed data from {data_path}\")\n",
    "    print(f\"  Train: {len(train_dataset)} examples\")\n",
    "    print(f\"  Dev:   {len(dev_dataset)} examples\")\n",
    "    print(f\"  Test:  {len(test_dataset)} examples\")\n",
    "    print(f\"  Labels: {len(label_mapping['label2id'])} unique tags\")\n",
    "    \n",
    "    return label_mapping, train_dataset, dev_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Load data\n",
    "label_mapping, train_dataset, dev_dataset, test_dataset = load_processed_data(PROCESSED_DATA_PATH)\n",
    "\n",
    "# Convert string keys to integers for model\n",
    "id2label = {int(k): v for k, v in label_mapping[\"id2label\"].items()}\n",
    "label2id = label_mapping[\"label2id\"]\n",
    "\n",
    "print(f\"\\nLabel mapping preview:\")\n",
    "for idx in sorted(id2label.keys())[:10]:\n",
    "    print(f\"  {idx}: {id2label[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization and Label Alignment\n",
    "\n",
    "### The Subword Problem\n",
    "\n",
    "**Critical concept:** Transformers use subword tokenization (WordPiece/BPE).\n",
    "\n",
    "Example:\n",
    "```\n",
    "Word:        \"Frankenstein\"  → Label: B-PER\n",
    "Subwords:    [\"Frank\", \"##en\", \"##stein\"]\n",
    "```\n",
    "\n",
    "**Question:** Which subwords get the label?\n",
    "\n",
    "**Solution:** We use a **first-token strategy**:\n",
    "- First subword (\"Frank\") → `B-PER` (1)\n",
    "- Continuation subwords (\"##en\", \"##stein\") → `-100` (ignored in loss)\n",
    "\n",
    "The `-100` label tells PyTorch to ignore these tokens when computing loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"✓ Loaded tokenizer: {MODEL_NAME}\")\n",
    "\n",
    "# Example: Show subword tokenization\n",
    "example_text = \"Dr. Frankenstein visited Switzerland\"\n",
    "tokens = tokenizer.tokenize(example_text)\n",
    "print(f\"\\nExample tokenization:\")\n",
    "print(f\"  Input:  {example_text}\")\n",
    "print(f\"  Tokens: {tokens}\")\n",
    "print(f\"\\nNotice how 'Frankenstein' and 'Switzerland' are split into subwords!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Tokenize input and align NER labels with subword tokens.\n",
    "    \n",
    "    WHY: This is the most critical function for NER with transformers!\n",
    "    Without proper alignment, your model will learn incorrect labels.\n",
    "    \n",
    "    Process:\n",
    "    1. Tokenize words into subwords\n",
    "    2. For each subword, determine which original word it came from\n",
    "    3. Assign labels:\n",
    "       - First subword of word → original label\n",
    "       - Continuation subwords → -100 (ignored)\n",
    "       - Special tokens ([CLS], [SEP]) → -100 (ignored)\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of examples with \"tokens\" and \"ner_tags\"\n",
    "        \n",
    "    Returns:\n",
    "        Tokenized inputs with aligned labels\n",
    "    \"\"\"\n",
    "    # Tokenize all examples\n",
    "    # is_split_into_words=True tells tokenizer we already have word tokens\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        is_split_into_words=True,\n",
    "        padding=False  # We'll pad dynamically in batches\n",
    "    )\n",
    "    \n",
    "    all_labels = []\n",
    "    \n",
    "    # Process each example in the batch\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        # word_ids() returns which original word each token came from\n",
    "        # Example: [None, 0, 1, 1, 2, None] means:\n",
    "        #   - Token 0: special token ([CLS])\n",
    "        #   - Token 1: from word 0\n",
    "        #   - Token 2-3: from word 1 (subwords!)\n",
    "        #   - Token 4: from word 2\n",
    "        #   - Token 5: special token ([SEP])\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens (None) → -100\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            # First subword of a word → original label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            \n",
    "            # Continuation subwords → -100\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        all_labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "print(\"✓ Tokenization function defined\")\n",
    "print(\"\\nKey concept: -100 labels are ignored during training\")\n",
    "print(\"This prevents the model from being penalized for subword predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to all datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing training data\"\n",
    ")\n",
    "\n",
    "tokenized_dev = dev_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names,\n",
    "    desc=\"Tokenizing dev data\"\n",
    ")\n",
    "\n",
    "tokenized_test = test_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    desc=\"Tokenizing test data\"\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Tokenization complete!\")\n",
    "print(f\"\\nExample tokenized input:\")\n",
    "example = tokenized_train[0]\n",
    "print(f\"  Input IDs shape: {len(example['input_ids'])}\")\n",
    "print(f\"  Labels shape:    {len(example['labels'])}\")\n",
    "print(f\"  First 10 tokens: {tokenizer.convert_ids_to_tokens(example['input_ids'][:10])}\")\n",
    "print(f\"  First 10 labels: {example['labels'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model\n",
    "\n",
    "We'll load XLM-RoBERTa and add a token classification head on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True  # Since we're adding a new classification head\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded model: {MODEL_NAME}\")\n",
    "print(f\"  Parameters: {model.num_parameters():,}\")\n",
    "print(f\"  Output labels: {model.num_labels}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  1. XLM-RoBERTa encoder (pre-trained on 100 languages)\")\n",
    "print(f\"  2. Token classification head (randomly initialized, will be trained)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Metrics\n",
    "\n",
    "**Why seqeval?**\n",
    "- Standard NER evaluation library\n",
    "- Computes entity-level F1 (not token-level)\n",
    "- Example: \"New York\" counted as 1 entity, not 2 tokens\n",
    "\n",
    "**Metrics:**\n",
    "- **Precision**: Of predicted entities, how many were correct?\n",
    "- **Recall**: Of true entities, how many did we find?\n",
    "- **F1**: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics during training.\n",
    "    \n",
    "    WHY: This function is called by Trainer after each evaluation.\n",
    "    It converts model predictions back to BIO tags and computes F1.\n",
    "    \n",
    "    Args:\n",
    "        pred: Predictions object with logits and labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, f1, accuracy\n",
    "    \"\"\"\n",
    "    predictions, labels = pred\n",
    "    \n",
    "    # Get predicted class for each token (argmax over logits)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Convert to BIO tag strings (removing -100 labels)\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        true_label = []\n",
    "        true_pred = []\n",
    "        \n",
    "        for pred_id, label_id in zip(prediction, label):\n",
    "            # Skip -100 labels (subwords and special tokens)\n",
    "            if label_id != -100:\n",
    "                true_label.append(id2label[label_id])\n",
    "                true_pred.append(id2label[pred_id])\n",
    "        \n",
    "        true_labels.append(true_label)\n",
    "        true_predictions.append(true_pred)\n",
    "    \n",
    "    # Compute entity-level metrics using seqeval\n",
    "    results = {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation metrics defined\")\n",
    "print(\"  Using seqeval for entity-level F1 scoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "**Key hyperparameters explained:**\n",
    "- **Learning rate**: How big the update steps are (2e-5 is standard for fine-tuning)\n",
    "- **Batch size**: Number of examples processed together (larger = faster but more memory)\n",
    "- **Epochs**: Full passes through training data (3-5 typical for NER)\n",
    "- **Warmup**: Gradually increase learning rate at start (stabilizes training)\n",
    "- **Weight decay**: Regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(MODEL_OUTPUT_PATH / \"checkpoints\"),\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,  # L2 regularization\n",
    "    \n",
    "    # Evaluation and logging\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",  # Save checkpoint after each epoch\n",
    "    logging_steps=100,      # Log every 100 steps\n",
    "    load_best_model_at_end=True,  # Load best checkpoint at end\n",
    "    metric_for_best_model=\"f1\",    # Use F1 to select best model\n",
    "    \n",
    "    # Performance optimizations\n",
    "    fp16=False,  # Set to True if you have a GPU with FP16 support\n",
    "    dataloader_num_workers=0,  # Increase if you have multiple CPU cores\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard (can enable if desired)\n",
    "    \n",
    "    # Other settings\n",
    "    push_to_hub=False,\n",
    "    seed=42,  # For reproducibility\n",
    ")\n",
    "\n",
    "# Data collator (handles dynamic padding)\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True  # Pad to longest sequence in batch\n",
    ")\n",
    "\n",
    "print(\"✓ Training configuration ready\")\n",
    "print(f\"\\nTotal training steps: ~{len(tokenized_train) // BATCH_SIZE * NUM_EPOCHS}\")\n",
    "print(f\"Evaluation: After each epoch\")\n",
    "print(f\"Best model selected by: F1 score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "**What happens during training:**\n",
    "1. Model reads tokenized text\n",
    "2. Predicts label probabilities for each token\n",
    "3. Compares predictions to true labels (cross-entropy loss)\n",
    "4. Updates weights via backpropagation\n",
    "5. Repeats for all batches (1 epoch)\n",
    "6. Evaluates on dev set\n",
    "7. Repeats for NUM_EPOCHS\n",
    "\n",
    "**Training time:** ~10-30 minutes on GPU, 1-2 hours on CPU (depending on dataset size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"This may take 10-60 minutes depending on your hardware.\")\n",
    "print(\"Progress bar will show training and evaluation metrics.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining metrics:\")\n",
    "print(f\"  Total training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"  Training loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Model\n",
    "\n",
    "We'll save the best model for use in Notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "final_model_path = MODEL_OUTPUT_PATH / \"litbank-xlm-roberta\"\n",
    "trainer.save_model(str(final_model_path))\n",
    "tokenizer.save_pretrained(str(final_model_path))\n",
    "\n",
    "print(f\"✓ Model saved to: {final_model_path.absolute()}\")\n",
    "print(\"\\nSaved files:\")\n",
    "for file in final_model_path.iterdir():\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Test Set\n",
    "\n",
    "**Important:** We only evaluate on the test set ONCE at the end.\n",
    "Using test data during development leads to overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS (English Literary Text)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOverall metrics:\")\n",
    "print(f\"  Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_results['eval_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Per-Entity Type Evaluation\n",
    "\n",
    "Let's see how well the model performs on each entity type (PER, LOC, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detailed_predictions(trainer, dataset, original_dataset):\n",
    "    \"\"\"\n",
    "    Get predictions and convert back to BIO tags for detailed analysis.\n",
    "    \n",
    "    Returns:\n",
    "        true_labels: List of label sequences\n",
    "        predictions: List of prediction sequences\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    pred_output = trainer.predict(dataset)\n",
    "    predictions = np.argmax(pred_output.predictions, axis=2)\n",
    "    \n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "    \n",
    "    # Convert to BIO tags\n",
    "    for i, (prediction, label) in enumerate(zip(predictions, pred_output.label_ids)):\n",
    "        true_label = []\n",
    "        true_pred = []\n",
    "        \n",
    "        for pred_id, label_id in zip(prediction, label):\n",
    "            if label_id != -100:\n",
    "                true_label.append(id2label[label_id])\n",
    "                true_pred.append(id2label[pred_id])\n",
    "        \n",
    "        true_labels.append(true_label)\n",
    "        true_predictions.append(true_pred)\n",
    "    \n",
    "    return true_labels, true_predictions\n",
    "\n",
    "\n",
    "# Get detailed predictions\n",
    "print(\"Generating detailed predictions...\")\n",
    "true_labels, predictions = get_detailed_predictions(trainer, tokenized_test, test_dataset)\n",
    "\n",
    "# Print classification report (per-entity metrics)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-ENTITY TYPE PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "\n",
    "# Save report\n",
    "report_text = classification_report(true_labels, predictions, digits=4)\n",
    "with open(RESULTS_PATH / \"test_classification_report.txt\", 'w') as f:\n",
    "    f.write(\"Test Set Classification Report\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(report_text)\n",
    "\n",
    "print(f\"\\n✓ Saved detailed report to {RESULTS_PATH / 'test_classification_report.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrix\n",
    "\n",
    "**Why confusion matrices?**\n",
    "Shows which entity types the model confuses:\n",
    "- Diagonal = correct predictions\n",
    "- Off-diagonal = errors\n",
    "\n",
    "Example insights:\n",
    "- Does the model confuse LOC and GPE?\n",
    "- Are PER entities often missed (predicted as O)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_labels: List[List[str]], \n",
    "                         predictions: List[List[str]], \n",
    "                         save_path: Path = None):\n",
    "    \"\"\"\n",
    "    Create confusion matrix for entity types (B- tags only).\n",
    "    \n",
    "    WHY: Focus on entity types (PER, LOC, etc.) rather than BIO positions.\n",
    "    \"\"\"\n",
    "    # Flatten lists and extract entity types\n",
    "    flat_true = []\n",
    "    flat_pred = []\n",
    "    \n",
    "    for true_seq, pred_seq in zip(true_labels, predictions):\n",
    "        flat_true.extend(true_seq)\n",
    "        flat_pred.extend(pred_seq)\n",
    "    \n",
    "    # Get unique labels (sorted)\n",
    "    unique_labels = sorted(set(flat_true + flat_pred))\n",
    "    \n",
    "    # For readability, focus on entity types (B- and I- separately)\n",
    "    # Or just top N most common labels\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(flat_true)\n",
    "    top_labels = [label for label, _ in label_counts.most_common(15)]  # Top 15 labels\n",
    "    \n",
    "    # Filter predictions and labels\n",
    "    filtered_true = [l if l in top_labels else 'OTHER' for l in flat_true]\n",
    "    filtered_pred = [l if l in top_labels else 'OTHER' for l in flat_pred]\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(filtered_true, filtered_pred, labels=top_labels + ['OTHER'])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=top_labels + ['OTHER'],\n",
    "        yticklabels=top_labels + ['OTHER'],\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.title('Confusion Matrix: Entity Recognition', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved confusion matrix to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    true_labels, \n",
    "    predictions,\n",
    "    save_path=RESULTS_PATH / \"confusion_matrix.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Curves\n",
    "\n",
    "Visualize learning progress over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(trainer, save_path: Path = None):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics over epochs.\n",
    "    \n",
    "    WHY: Helps diagnose:\n",
    "    - Overfitting (train improves, val plateaus)\n",
    "    - Underfitting (both metrics poor)\n",
    "    - Convergence (metrics stabilize)\n",
    "    \"\"\"\n",
    "    # Extract metrics from log history\n",
    "    log_history = trainer.state.log_history\n",
    "    \n",
    "    # Separate training and evaluation logs\n",
    "    train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "    eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Loss curves\n",
    "    if train_logs and eval_logs:\n",
    "        train_steps = [log['step'] for log in train_logs if 'loss' in log]\n",
    "        train_loss = [log['loss'] for log in train_logs if 'loss' in log]\n",
    "        \n",
    "        eval_epochs = [log['epoch'] for log in eval_logs]\n",
    "        eval_loss = [log['eval_loss'] for log in eval_logs]\n",
    "        \n",
    "        axes[0].plot(train_steps, train_loss, label='Training Loss', marker='o', markersize=3)\n",
    "        # Align eval_loss with train steps (approximate)\n",
    "        if train_steps and eval_epochs:\n",
    "            steps_per_epoch = max(train_steps) / max(eval_epochs) if eval_epochs else 1\n",
    "            eval_steps = [epoch * steps_per_epoch for epoch in eval_epochs]\n",
    "            axes[0].plot(eval_steps, eval_loss, label='Validation Loss', marker='s', markersize=6)\n",
    "        \n",
    "        axes[0].set_xlabel('Training Steps', fontsize=11)\n",
    "        axes[0].set_ylabel('Loss', fontsize=11)\n",
    "        axes[0].set_title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 2: F1 Score progression\n",
    "    if eval_logs:\n",
    "        eval_epochs = [log['epoch'] for log in eval_logs]\n",
    "        eval_f1 = [log.get('eval_f1', 0) for log in eval_logs]\n",
    "        eval_precision = [log.get('eval_precision', 0) for log in eval_logs]\n",
    "        eval_recall = [log.get('eval_recall', 0) for log in eval_logs]\n",
    "        \n",
    "        axes[1].plot(eval_epochs, eval_f1, label='F1 Score', marker='o', linewidth=2)\n",
    "        axes[1].plot(eval_epochs, eval_precision, label='Precision', marker='s', linestyle='--')\n",
    "        axes[1].plot(eval_epochs, eval_recall, label='Recall', marker='^', linestyle='--')\n",
    "        \n",
    "        axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "        axes[1].set_ylabel('Score', fontsize=11)\n",
    "        axes[1].set_title('Validation Metrics', fontsize=12, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(alpha=0.3)\n",
    "        axes[1].set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved training curves to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot training curves\n",
    "plot_training_curves(\n",
    "    trainer,\n",
    "    save_path=RESULTS_PATH / \"training_curves.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Example Predictions\n",
    "\n",
    "Let's visualize some predictions to understand model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(tokens: List[str], true_labels: List[str], predictions: List[str], num_examples: int = 3):\n",
    "    \"\"\"\n",
    "    Display predictions with color-coded entities.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE PREDICTIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i in range(min(num_examples, len(tokens))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        token_seq = tokens[i]\n",
    "        true_seq = true_labels[i]\n",
    "        pred_seq = predictions[i]\n",
    "        \n",
    "        # Display first 50 tokens\n",
    "        display_len = min(50, len(token_seq))\n",
    "        \n",
    "        for j in range(display_len):\n",
    "            token = token_seq[j]\n",
    "            true_label = true_seq[j]\n",
    "            pred_label = pred_seq[j]\n",
    "            \n",
    "            # Mark correct/incorrect\n",
    "            marker = \"✓\" if true_label == pred_label else \"✗\"\n",
    "            \n",
    "            if true_label != 'O' or pred_label != 'O':\n",
    "                print(f\"  {token:20s} | True: {true_label:12s} | Pred: {pred_label:12s} {marker}\")\n",
    "        \n",
    "        if len(token_seq) > display_len:\n",
    "            print(f\"  ... ({len(token_seq) - display_len} more tokens)\")\n",
    "\n",
    "\n",
    "# Show examples\n",
    "show_predictions(\n",
    "    [ex['tokens'] for ex in test_dataset],\n",
    "    true_labels,\n",
    "    predictions,\n",
    "    num_examples=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "✓ Loaded preprocessed LitBank data  \n",
    "✓ Tokenized with subword alignment (critical for transformers!)  \n",
    "✓ Fine-tuned XLM-RoBERTa for token classification  \n",
    "✓ Evaluated on English literary test set  \n",
    "✓ Generated per-entity F1 scores  \n",
    "✓ Created confusion matrix and training curves  \n",
    "✓ Saved model for multilingual evaluation  \n",
    "\n",
    "**Key concepts learned:**\n",
    "1. **Subword tokenization**: Why -100 labels matter\n",
    "2. **Label alignment**: First-token strategy for NER\n",
    "3. **Entity-level metrics**: seqeval F1 vs token-level accuracy\n",
    "4. **Training dynamics**: Loss curves and convergence\n",
    "\n",
    "**Files created:**\n",
    "- `models/litbank-xlm-roberta/` - Fine-tuned model\n",
    "- `results/test_classification_report.txt` - Detailed metrics\n",
    "- `results/confusion_matrix.png` - Error analysis\n",
    "- `results/training_curves.png` - Learning progression\n",
    "\n",
    "**Next steps:**\n",
    "Proceed to **Notebook 3: Multilingual Evaluation** to test cross-lingual transfer!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
