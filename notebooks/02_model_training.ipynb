{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning XLM-RoBERTa for Multilingual NER - Sequential Training\n",
    "\n",
    "## Overview\n",
    "This notebook fine-tunes a multilingual transformer model using **sequential language training** on English, French, and Italian LitBank datasets.\n",
    "\n",
    "**Training Approach: Sequential Language Addition**\n",
    "- Train on English data → Evaluate on all three test sets\n",
    "- Continue training on French data (keeping EN weights) → Evaluate on all three test sets  \n",
    "- Continue training on Italian data (keeping EN+FR weights) → Evaluate on all three test sets\n",
    "\n",
    "This approach allows us to observe how the model's cross-lingual performance evolves as we incrementally add training data from each language.\n",
    "\n",
    "**Content**\n",
    "1. Load preprocessed English, French, and Italian data from Notebook 1\n",
    "2. Configure XLM-RoBERTa for token classification\n",
    "3. Handle subword tokenization\n",
    "4. Sequential training: EN → FR → IT\n",
    "5. Evaluate on all three test sets after each training stage\n",
    "6. Visualize performance evolution across training stages\n",
    "7. Compare cross-lingual transfer effects\n",
    "\n",
    "**Model: XLM-RoBERTa**\n",
    "- Multilingual (Pre-trained on 100 languages)\n",
    "- State-of-the-art performance on sequence labeling\n",
    "- Enables cross-lingual transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Evaluation metrics\n",
    "from seqeval.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Model: xlm-roberta-base\n",
      "  Max length: 512 tokens\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n",
      "  Epochs: 4\n"
     ]
    }
   ],
   "source": [
    "# Configure paths\n",
    "PROCESSED_DATA_PATH = Path(\"../data/processed\")\n",
    "MODEL_OUTPUT_PATH = Path(\"../models\")\n",
    "RESULTS_PATH = Path(\"../results\")\n",
    "\n",
    "MODEL_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"xlm-roberta-base\"  # Multilingual model\n",
    "MAX_LENGTH = 512  # Maximum sequence length (tokens)\n",
    "BATCH_SIZE = 16   # Adjust based on your GPU memory (8/16/32)\n",
    "LEARNING_RATE = 2e-5  # Standard for fine-tuning transformers\n",
    "NUM_EPOCHS = 4    # Typically 3-5 epochs for NER\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Max length: {MAX_LENGTH} tokens\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "Load the JSON files created in Notebook 1. For sequential training, we keep the language datasets separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(data_path: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Load preprocessed JSON data and convert to Hugging Face Dataset format.\n",
    "    Returns separate datasets for each language for sequential training.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - label_mapping: {\"label2id\": {...}, \"id2label\": {...}}\n",
    "        - english: {\"train\": Dataset, \"dev\": Dataset, \"test\": Dataset}\n",
    "        - french: {\"train\": Dataset, \"dev\": Dataset, \"test\": Dataset}\n",
    "        - italian: {\"train\": Dataset, \"dev\": Dataset, \"test\": Dataset}\n",
    "    \"\"\"\n",
    "    # Load label mapping\n",
    "    with open(data_path / \"label_mapping.json\", 'r', encoding='utf-8') as f:\n",
    "        label_mapping = json.load(f)\n",
    "    \n",
    "    # Load English datasets\n",
    "    with open(data_path / \"english_train.json\", 'r', encoding='utf-8') as f:\n",
    "        english_train = json.load(f)\n",
    "    with open(data_path / \"english_dev.json\", 'r', encoding='utf-8') as f:\n",
    "        english_dev = json.load(f)\n",
    "    with open(data_path / \"english_test.json\", 'r', encoding='utf-8') as f:\n",
    "        english_test = json.load(f)\n",
    "    \n",
    "    # Load French datasets\n",
    "    with open(data_path / \"french_train.json\", 'r', encoding='utf-8') as f:\n",
    "        french_train = json.load(f)\n",
    "    with open(data_path / \"french_dev.json\", 'r', encoding='utf-8') as f:\n",
    "        french_dev = json.load(f)\n",
    "    with open(data_path / \"french_test.json\", 'r', encoding='utf-8') as f:\n",
    "        french_test = json.load(f)\n",
    "    \n",
    "    # Load Italian datasets\n",
    "    with open(data_path / \"italian_train.json\", 'r', encoding='utf-8') as f:\n",
    "        italian_train = json.load(f)\n",
    "    with open(data_path / \"italian_dev.json\", 'r', encoding='utf-8') as f:\n",
    "        italian_dev = json.load(f)\n",
    "    with open(data_path / \"italian_test.json\", 'r', encoding='utf-8') as f:\n",
    "        italian_test = json.load(f)\n",
    "    \n",
    "    # Convert to Hugging Face Dataset format\n",
    "    data = {\n",
    "        \"label_mapping\": label_mapping,\n",
    "        \"english\": {\n",
    "            \"train\": Dataset.from_list(english_train),\n",
    "            \"dev\": Dataset.from_list(english_dev),\n",
    "            \"test\": Dataset.from_list(english_test),\n",
    "            \"raw_test\": english_test  # Keep raw for visualization\n",
    "        },\n",
    "        \"french\": {\n",
    "            \"train\": Dataset.from_list(french_train),\n",
    "            \"dev\": Dataset.from_list(french_dev),\n",
    "            \"test\": Dataset.from_list(french_test),\n",
    "            \"raw_test\": french_test\n",
    "        },\n",
    "        \"italian\": {\n",
    "            \"train\": Dataset.from_list(italian_train),\n",
    "            \"dev\": Dataset.from_list(italian_dev),\n",
    "            \"test\": Dataset.from_list(italian_test),\n",
    "            \"raw_test\": italian_test\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Loaded preprocessed data from {data_path}\")\n",
    "    print(f\"\\nEnglish datasets:\")\n",
    "    print(f\"  Train: {len(english_train)} examples\")\n",
    "    print(f\"  Dev:   {len(english_dev)} examples\")\n",
    "    print(f\"  Test:  {len(english_test)} examples\")\n",
    "    print(f\"\\nFrench datasets:\")\n",
    "    print(f\"  Train: {len(french_train)} examples\")\n",
    "    print(f\"  Dev:   {len(french_dev)} examples\")\n",
    "    print(f\"  Test:  {len(french_test)} examples\")\n",
    "    print(f\"\\nItalian datasets:\")\n",
    "    print(f\"  Train: {len(italian_train)} examples\")\n",
    "    print(f\"  Dev:   {len(italian_dev)} examples\")\n",
    "    print(f\"  Test:  {len(italian_test)} examples\")\n",
    "    print(f\"\\nLabels: {len(label_mapping['label2id'])} unique tags\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = load_processed_data(PROCESSED_DATA_PATH)\n",
    "\n",
    "# Extract label mappings\n",
    "label_mapping = data[\"label_mapping\"]\n",
    "id2label = {int(k): v for k, v in label_mapping[\"id2label\"].items()}\n",
    "label2id = label_mapping[\"label2id\"]\n",
    "\n",
    "print(f\"\\nLabel mapping preview:\")\n",
    "for idx in sorted(id2label.keys())[:10]:\n",
    "    print(f\"  {idx}: {id2label[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization and Label Alignment\n",
    "\n",
    "Transformers use subword tokenization (WordPiece/BPE), while labels correspond to words. Which subwords get the label?\n",
    "\n",
    "**First-token strategy** : the first subword receives the label, subsequent tokens will be ignored (the `-100` label tells PyTorch to ignore these tokens when computing loss.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded tokenizer: xlm-roberta-base\n",
      "\n",
      "Example tokenization:\n",
      "  Input:  Elizabeth Bennet lived in Pemberley in the lake district of northern England.\n",
      "  Tokens: ['▁Elizabeth', '▁Ben', 'net', '▁lived', '▁in', '▁Pembe', 'r', 'ley', '▁in', '▁the', '▁lake', '▁district', '▁of', '▁north', 'ern', '▁England', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"✓ Loaded tokenizer: {MODEL_NAME}\")\n",
    "\n",
    "# Example: Show subword tokenization\n",
    "example_text = \"Elizabeth Bennet lived in Pemberley in the lake district of northern England.\"\n",
    "tokens = tokenizer.tokenize(example_text)\n",
    "print(f\"\\nExample tokenization:\")\n",
    "print(f\"  Input:  {example_text}\")\n",
    "print(f\"  Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenization function defined\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Tokenize input and align NER labels with subword tokens.\n",
    "    \n",
    "    WHY: This is the most critical function for NER with transformers!\n",
    "    Without proper alignment, your model will learn incorrect labels.\n",
    "    \n",
    "    Process:\n",
    "    1. Tokenize words into subwords\n",
    "    2. For each subword, determine which original word it came from\n",
    "    3. Assign labels:\n",
    "       - First subword of word → original label\n",
    "       - Continuation subwords → -100 (ignored)\n",
    "       - Special tokens ([CLS], [SEP]) → -100 (ignored)\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of examples with \"tokens\" and \"ner_tags\"\n",
    "        \n",
    "    Returns:\n",
    "        Tokenized inputs with aligned labels\n",
    "    \"\"\"\n",
    "    # Tokenize all examples\n",
    "    # is_split_into_words=True tells tokenizer we already have word tokens\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        is_split_into_words=True,\n",
    "        padding=False  # We'll pad dynamically in batches\n",
    "    )\n",
    "    \n",
    "    all_labels = []\n",
    "    \n",
    "    # Process each example in the batch\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        # word_ids() returns which original word each token came from\n",
    "        # Example: [None, 0, 1, 1, 2, None] means:\n",
    "        #   - Token 0: special token ([CLS])\n",
    "        #   - Token 1: from word 0\n",
    "        #   - Token 2-3: from word 1 (subwords!)\n",
    "        #   - Token 4: from word 2\n",
    "        #   - Token 5: special token ([SEP])\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens (None) → -100\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            # First subword of a word → original label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            \n",
    "            # Continuation subwords → -100\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        all_labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "print(\"✓ Tokenization function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to all datasets for each language\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "# Tokenize English datasets\n",
    "tokenized_english = {\n",
    "    \"train\": data[\"english\"][\"train\"].map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=data[\"english\"][\"train\"].column_names,\n",
    "        desc=\"Tokenizing English train\"\n",
    "    ),\n",
    "    \"dev\": data[\"english\"][\"dev\"].map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=data[\"english\"][\"dev\"].column_names,\n",
    "        desc=\"Tokenizing English dev\"\n",
    "    ),\n",
    "    \"test\": data[\"english\"][\"test\"].map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=data[\"english\"][\"test\"].column_names,\n",
    "        desc=\"Tokenizing English test\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Tokenize French datasets\n",
    "tokenized_french = {\n",
    "    \"train\": data[\"french\"][\"train\"].map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=data[\"french\"][\"train\"].column_names,\n",
    "        desc=\"Tokenizing French train\"\n",
    "    ),\n",
    "    \"dev\": data[\"french\"][\"dev\"].map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=data[\"french\"][\"dev\"].column_names,\n",
    "        desc=\"Tokenizing French dev\"\n",
    "    ),\n",
    "    \"test\": data[\"french\"][\"test\"].map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=data[\"french\"][\"test\"].column_names,\n",
    "        desc=\"Tokenizing French test\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Tokenize Italian datasets\n",
    "tokenized_italian = {\n",
    "    \"train\": data[\"italian\"][\"train\"].map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=data[\"italian\"][\"train\"].column_names,\n",
    "        desc=\"Tokenizing Italian train\"\n",
    "    ),\n",
    "    \"dev\": data[\"italian\"][\"dev\"].map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=data[\"italian\"][\"dev\"].column_names,\n",
    "        desc=\"Tokenizing Italian dev\"\n",
    "    ),\n",
    "    \"test\": data[\"italian\"][\"test\"].map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=data[\"italian\"][\"test\"].column_names,\n",
    "        desc=\"Tokenizing Italian test\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Tokenization complete!\")\n",
    "print(f\"\\nDataset sizes after tokenization:\")\n",
    "print(f\"  English - Train: {len(tokenized_english['train'])}, Dev: {len(tokenized_english['dev'])}, Test: {len(tokenized_english['test'])}\")\n",
    "print(f\"  French  - Train: {len(tokenized_french['train'])}, Dev: {len(tokenized_french['dev'])}, Test: {len(tokenized_french['test'])}\")\n",
    "print(f\"  Italian - Train: {len(tokenized_italian['train'])}, Dev: {len(tokenized_italian['dev'])}, Test: {len(tokenized_italian['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model\n",
    "\n",
    "Load XLM-RoBERTa and add a token classification head on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model: xlm-roberta-base\n",
      "  Parameters: 277,464,591\n",
      "  Output labels: 15\n",
      "\n",
      "Model architecture:\n",
      "  1. XLM-RoBERTa encoder (pre-trained on 100 languages)\n",
      "  2. Token classification head (randomly initialized, will be trained)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True  # Since we're adding a new classification head\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded model: {MODEL_NAME}\")\n",
    "print(f\"  Parameters: {model.num_parameters():,}\")\n",
    "print(f\"  Output labels: {model.num_labels}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  1. XLM-RoBERTa encoder (pre-trained on 100 languages)\")\n",
    "print(f\"  2. Token classification head (randomly initialized, will be trained)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Metrics\n",
    "\n",
    "**Seqeval**\n",
    "- Standard NER evaluation library\n",
    "- Computes entity-level F1 (not token-level)\n",
    "\n",
    "**Metrics:**\n",
    "- **Precision**: Of predicted entities, how many were correct?\n",
    "- **Recall**: Of true entities, how many were found?\n",
    "- **F1**: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation metrics defined\n",
      "  Using seqeval for entity-level F1 scoring\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics during training.\n",
    "    \n",
    "    WHY: This function is called by Trainer after each evaluation.\n",
    "    It converts model predictions back to BIO tags and computes F1.\n",
    "    \n",
    "    Args:\n",
    "        pred: Predictions object with logits and labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, f1, accuracy\n",
    "    \"\"\"\n",
    "    predictions, labels = pred\n",
    "    \n",
    "    # Get predicted class for each token (argmax over logits)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Convert to BIO tag strings (removing -100 labels)\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        true_label = []\n",
    "        true_pred = []\n",
    "        \n",
    "        for pred_id, label_id in zip(prediction, label):\n",
    "            # Skip -100 labels (subwords and special tokens)\n",
    "            if label_id != -100:\n",
    "                true_label.append(id2label[label_id])\n",
    "                true_pred.append(id2label[pred_id])\n",
    "        \n",
    "        true_labels.append(true_label)\n",
    "        true_predictions.append(true_pred)\n",
    "    \n",
    "    # Compute entity-level metrics using seqeval\n",
    "    results = {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation metrics defined\")\n",
    "print(\"  Using seqeval for entity-level F1 scoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "**Key hyperparameters**\n",
    "- **Learning rate**: How big the update steps are (2e-5 is standard for fine-tuning)\n",
    "- **Batch size**: Number of examples processed together (larger = faster but more memory)\n",
    "- **Epochs**: Full passes through training data (3-5 typical for NER)\n",
    "- **Warmup**: Gradually increase learning rate at start (stabilizes training)\n",
    "- **Weight decay**: Regularization to prevent overfitting\n",
    "\n",
    "**Sequential Training Setup**\n",
    "We'll train on each language in sequence (EN → FR → IT), evaluating on all test sets after each stage to track how performance evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator (handles dynamic padding)\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True  # Pad to longest sequence in batch\n",
    ")\n",
    "\n",
    "# Store results for visualization\n",
    "sequential_results = {\n",
    "    \"stages\": [],  # [\"After EN\", \"After EN+FR\", \"After EN+FR+IT\"]\n",
    "    \"english_test\": {\"precision\": [], \"recall\": [], \"f1\": []},\n",
    "    \"french_test\": {\"precision\": [], \"recall\": [], \"f1\": []},\n",
    "    \"italian_test\": {\"precision\": [], \"recall\": [], \"f1\": []}\n",
    "}\n",
    "\n",
    "def get_training_args(stage_name: str, output_subdir: str):\n",
    "    \"\"\"Create training arguments for a specific training stage.\"\"\"\n",
    "    return TrainingArguments(\n",
    "        output_dir=str(MODEL_OUTPUT_PATH / \"checkpoints\" / output_subdir),\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        # Evaluation and logging\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        \n",
    "        # Performance\n",
    "        fp16=False,\n",
    "        dataloader_num_workers=0,\n",
    "        \n",
    "        # Reporting\n",
    "        report_to=\"none\",\n",
    "        push_to_hub=False,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_on_all_test_sets(trainer, stage_name: str):\n",
    "    \"\"\"\n",
    "    Evaluate model on all three test sets and store results.\n",
    "    \n",
    "    Args:\n",
    "        trainer: The Trainer object with current model\n",
    "        stage_name: Name of the training stage (e.g., \"After EN\")\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results for each language\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate on English test set\n",
    "    en_results = trainer.evaluate(eval_dataset=tokenized_english[\"test\"])\n",
    "    results[\"english\"] = {\n",
    "        \"precision\": en_results[\"eval_precision\"],\n",
    "        \"recall\": en_results[\"eval_recall\"],\n",
    "        \"f1\": en_results[\"eval_f1\"]\n",
    "    }\n",
    "    \n",
    "    # Evaluate on French test set\n",
    "    fr_results = trainer.evaluate(eval_dataset=tokenized_french[\"test\"])\n",
    "    results[\"french\"] = {\n",
    "        \"precision\": fr_results[\"eval_precision\"],\n",
    "        \"recall\": fr_results[\"eval_recall\"],\n",
    "        \"f1\": fr_results[\"eval_f1\"]\n",
    "    }\n",
    "    \n",
    "    # Evaluate on Italian test set\n",
    "    it_results = trainer.evaluate(eval_dataset=tokenized_italian[\"test\"])\n",
    "    results[\"italian\"] = {\n",
    "        \"precision\": it_results[\"eval_precision\"],\n",
    "        \"recall\": it_results[\"eval_recall\"],\n",
    "        \"f1\": it_results[\"eval_f1\"]\n",
    "    }\n",
    "    \n",
    "    # Store in global results\n",
    "    sequential_results[\"stages\"].append(stage_name)\n",
    "    for lang in [\"english\", \"french\", \"italian\"]:\n",
    "        for metric in [\"precision\", \"recall\", \"f1\"]:\n",
    "            sequential_results[f\"{lang}_test\"][metric].append(results[lang][metric])\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATION RESULTS: {stage_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\n{'Language':<15} {'Precision':<12} {'Recall':<12} {'F1 Score':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'English':<15} {results['english']['precision']:<12.4f} {results['english']['recall']:<12.4f} {results['english']['f1']:<12.4f}\")\n",
    "    print(f\"{'French':<15} {results['french']['precision']:<12.4f} {results['french']['recall']:<12.4f} {results['french']['f1']:<12.4f}\")\n",
    "    print(f\"{'Italian':<15} {results['italian']['precision']:<12.4f} {results['italian']['recall']:<12.4f} {results['italian']['f1']:<12.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✓ Training configuration ready\")\n",
    "print(f\"  Epochs per language: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sequential Training\n",
    "\n",
    "**Training sequence:**\n",
    "1. Train on English data → Evaluate on EN, FR, IT test sets\n",
    "2. Continue training on French data → Evaluate on EN, FR, IT test sets\n",
    "3. Continue training on Italian data → Evaluate on EN, FR, IT test sets\n",
    "\n",
    "The model weights are preserved between stages, allowing us to observe cross-lingual transfer effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Train on English\n",
    "print(\"=\"*60)\n",
    "print(\"STAGE 1: Training on English\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize fresh model from pre-trained weights\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded fresh model: {MODEL_NAME}\")\n",
    "print(f\"  Parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# Create trainer for English\n",
    "training_args_en = get_training_args(\"English\", \"stage1_english\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_en,\n",
    "    train_dataset=tokenized_english[\"train\"],\n",
    "    eval_dataset=tokenized_english[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized for English training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on English\n",
    "print(\"\\nStarting English training...\")\n",
    "train_result_en = trainer.train()\n",
    "\n",
    "print(f\"\\n✓ English training complete!\")\n",
    "print(f\"  Training loss: {train_result_en.metrics['train_loss']:.4f}\")\n",
    "\n",
    "# Evaluate on all test sets after English training\n",
    "results_after_en = evaluate_on_all_test_sets(trainer, \"After EN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Continue training on French (keeping English weights)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2: Training on French (continuing from EN weights)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create new trainer with French data, using the current model (with EN weights)\n",
    "training_args_fr = get_training_args(\"French\", \"stage2_french\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # Same model with EN weights\n",
    "    args=training_args_fr,\n",
    "    train_dataset=tokenized_french[\"train\"],\n",
    "    eval_dataset=tokenized_french[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train on French\n",
    "print(\"\\nStarting French training...\")\n",
    "train_result_fr = trainer.train()\n",
    "\n",
    "print(f\"\\n✓ French training complete!\")\n",
    "print(f\"  Training loss: {train_result_fr.metrics['train_loss']:.4f}\")\n",
    "\n",
    "# Evaluate on all test sets after French training\n",
    "results_after_fr = evaluate_on_all_test_sets(trainer, \"After EN+FR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: Continue training on Italian (keeping EN+FR weights)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 3: Training on Italian (continuing from EN+FR weights)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create new trainer with Italian data, using the current model (with EN+FR weights)\n",
    "training_args_it = get_training_args(\"Italian\", \"stage3_italian\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # Same model with EN+FR weights\n",
    "    args=training_args_it,\n",
    "    train_dataset=tokenized_italian[\"train\"],\n",
    "    eval_dataset=tokenized_italian[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train on Italian\n",
    "print(\"\\nStarting Italian training...\")\n",
    "train_result_it = trainer.train()\n",
    "\n",
    "print(f\"\\n✓ Italian training complete!\")\n",
    "print(f\"  Training loss: {train_result_it.metrics['train_loss']:.4f}\")\n",
    "\n",
    "# Evaluate on all test sets after Italian training\n",
    "results_after_it = evaluate_on_all_test_sets(trainer, \"After EN+FR+IT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Model\n",
    "\n",
    "Save the final model (trained on EN → FR → IT) for use in Notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model and tokenizer\n",
    "final_model_path = MODEL_OUTPUT_PATH / \"litbank-xlm-roberta-sequential\"\n",
    "trainer.save_model(str(final_model_path))\n",
    "tokenizer.save_pretrained(str(final_model_path))\n",
    "\n",
    "print(f\"✓ Final model saved to: {final_model_path.absolute()}\")\n",
    "print(\"\\nSaved files:\")\n",
    "for file in final_model_path.iterdir():\n",
    "    print(f\"  - {file.name}\")\n",
    "\n",
    "# Also save the sequential results to JSON for later analysis\n",
    "import json\n",
    "with open(RESULTS_PATH / \"sequential_training_results.json\", 'w') as f:\n",
    "    json.dump(sequential_results, f, indent=2)\n",
    "print(f\"\\n✓ Results saved to: {RESULTS_PATH / 'sequential_training_results.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Performance Evolution\n",
    "\n",
    "Visualize how precision, recall, and F1 scores evolve across training stages for each language test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of performance evolution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "stages = sequential_results[\"stages\"]\n",
    "x = np.arange(len(stages))\n",
    "width = 0.25\n",
    "\n",
    "colors = {\n",
    "    \"english\": \"#1f77b4\",  # Blue\n",
    "    \"french\": \"#ff7f0e\",   # Orange\n",
    "    \"italian\": \"#2ca02c\"   # Green\n",
    "}\n",
    "\n",
    "# Plot 1: F1 Score Evolution (Main metric)\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(stages, sequential_results[\"english_test\"][\"f1\"], \n",
    "         marker='o', linewidth=2, markersize=8, label='English Test', color=colors[\"english\"])\n",
    "ax1.plot(stages, sequential_results[\"french_test\"][\"f1\"], \n",
    "         marker='s', linewidth=2, markersize=8, label='French Test', color=colors[\"french\"])\n",
    "ax1.plot(stages, sequential_results[\"italian_test\"][\"f1\"], \n",
    "         marker='^', linewidth=2, markersize=8, label='Italian Test', color=colors[\"italian\"])\n",
    "ax1.set_xlabel('Training Stage', fontsize=11)\n",
    "ax1.set_ylabel('F1 Score', fontsize=11)\n",
    "ax1.set_title('F1 Score Evolution Across Training Stages', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Plot 2: Precision Evolution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(stages, sequential_results[\"english_test\"][\"precision\"], \n",
    "         marker='o', linewidth=2, markersize=8, label='English Test', color=colors[\"english\"])\n",
    "ax2.plot(stages, sequential_results[\"french_test\"][\"precision\"], \n",
    "         marker='s', linewidth=2, markersize=8, label='French Test', color=colors[\"french\"])\n",
    "ax2.plot(stages, sequential_results[\"italian_test\"][\"precision\"], \n",
    "         marker='^', linewidth=2, markersize=8, label='Italian Test', color=colors[\"italian\"])\n",
    "ax2.set_xlabel('Training Stage', fontsize=11)\n",
    "ax2.set_ylabel('Precision', fontsize=11)\n",
    "ax2.set_title('Precision Evolution Across Training Stages', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='best')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# Plot 3: Recall Evolution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(stages, sequential_results[\"english_test\"][\"recall\"], \n",
    "         marker='o', linewidth=2, markersize=8, label='English Test', color=colors[\"english\"])\n",
    "ax3.plot(stages, sequential_results[\"french_test\"][\"recall\"], \n",
    "         marker='s', linewidth=2, markersize=8, label='French Test', color=colors[\"french\"])\n",
    "ax3.plot(stages, sequential_results[\"italian_test\"][\"recall\"], \n",
    "         marker='^', linewidth=2, markersize=8, label='Italian Test', color=colors[\"italian\"])\n",
    "ax3.set_xlabel('Training Stage', fontsize=11)\n",
    "ax3.set_ylabel('Recall', fontsize=11)\n",
    "ax3.set_title('Recall Evolution Across Training Stages', fontsize=12, fontweight='bold')\n",
    "ax3.legend(loc='best')\n",
    "ax3.grid(alpha=0.3)\n",
    "ax3.set_ylim([0, 1])\n",
    "\n",
    "# Plot 4: Grouped Bar Chart - Final F1 Comparison\n",
    "ax4 = axes[1, 1]\n",
    "final_f1 = {\n",
    "    \"English Test\": [sequential_results[\"english_test\"][\"f1\"][i] for i in range(len(stages))],\n",
    "    \"French Test\": [sequential_results[\"french_test\"][\"f1\"][i] for i in range(len(stages))],\n",
    "    \"Italian Test\": [sequential_results[\"italian_test\"][\"f1\"][i] for i in range(len(stages))]\n",
    "}\n",
    "\n",
    "x_bar = np.arange(len(stages))\n",
    "ax4.bar(x_bar - width, final_f1[\"English Test\"], width, label='English Test', color=colors[\"english\"], alpha=0.8)\n",
    "ax4.bar(x_bar, final_f1[\"French Test\"], width, label='French Test', color=colors[\"french\"], alpha=0.8)\n",
    "ax4.bar(x_bar + width, final_f1[\"Italian Test\"], width, label='Italian Test', color=colors[\"italian\"], alpha=0.8)\n",
    "ax4.set_xlabel('Training Stage', fontsize=11)\n",
    "ax4.set_ylabel('F1 Score', fontsize=11)\n",
    "ax4.set_title('F1 Score Comparison by Training Stage', fontsize=12, fontweight='bold')\n",
    "ax4.set_xticks(x_bar)\n",
    "ax4.set_xticklabels(stages, rotation=15)\n",
    "ax4.legend(loc='best')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "ax4.set_ylim([0, 1])\n",
    "\n",
    "plt.suptitle('Sequential Training: Performance Evolution\\n(EN → FR → IT)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / \"sequential_training_evolution.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved visualization to {RESULTS_PATH / 'sequential_training_evolution.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed summary table\n",
    "print(\"=\"*80)\n",
    "print(\"SEQUENTIAL TRAINING RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"F1 SCORES BY STAGE\")\n",
    "print(\"-\"*80)\n",
    "print(f\"\\n{'Stage':<20} {'English Test':<15} {'French Test':<15} {'Italian Test':<15}\")\n",
    "print(\"-\"*65)\n",
    "for i, stage in enumerate(sequential_results[\"stages\"]):\n",
    "    en_f1 = sequential_results[\"english_test\"][\"f1\"][i]\n",
    "    fr_f1 = sequential_results[\"french_test\"][\"f1\"][i]\n",
    "    it_f1 = sequential_results[\"italian_test\"][\"f1\"][i]\n",
    "    print(f\"{stage:<20} {en_f1:<15.4f} {fr_f1:<15.4f} {it_f1:<15.4f}\")\n",
    "\n",
    "# Calculate and display changes\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"F1 SCORE CHANGES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def calc_change(values):\n",
    "    \"\"\"Calculate change from first to last value.\"\"\"\n",
    "    return values[-1] - values[0]\n",
    "\n",
    "def format_change(val):\n",
    "    \"\"\"Format change with sign and color indicator.\"\"\"\n",
    "    sign = \"+\" if val >= 0 else \"\"\n",
    "    return f\"{sign}{val:.4f}\"\n",
    "\n",
    "print(f\"\\nChange from 'After EN' to 'After EN+FR+IT':\")\n",
    "print(f\"  English Test: {format_change(calc_change(sequential_results['english_test']['f1']))}\")\n",
    "print(f\"  French Test:  {format_change(calc_change(sequential_results['french_test']['f1']))}\")\n",
    "print(f\"  Italian Test: {format_change(calc_change(sequential_results['italian_test']['f1']))}\")\n",
    "\n",
    "# Insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for catastrophic forgetting on English\n",
    "en_f1_change = calc_change(sequential_results[\"english_test\"][\"f1\"])\n",
    "if en_f1_change < -0.05:\n",
    "    print(f\"\\n⚠ Potential catastrophic forgetting on English: F1 dropped by {abs(en_f1_change):.4f}\")\n",
    "elif en_f1_change > 0.02:\n",
    "    print(f\"\\n✓ Positive transfer to English: F1 improved by {en_f1_change:.4f}\")\n",
    "else:\n",
    "    print(f\"\\n○ English performance relatively stable (change: {en_f1_change:.4f})\")\n",
    "\n",
    "# Check cross-lingual transfer\n",
    "fr_after_en = sequential_results[\"french_test\"][\"f1\"][0]\n",
    "it_after_en = sequential_results[\"italian_test\"][\"f1\"][0]\n",
    "\n",
    "print(f\"\\n• Zero-shot performance after English-only training:\")\n",
    "print(f\"  French Test F1: {fr_after_en:.4f}\")\n",
    "print(f\"  Italian Test F1: {it_after_en:.4f}\")\n",
    "\n",
    "if fr_after_en > 0.3:\n",
    "    print(\"  → Significant cross-lingual transfer from English to French\")\n",
    "if it_after_en > 0.3:\n",
    "    print(\"  → Significant cross-lingual transfer from English to Italian\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap visualization of all metrics across stages\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame for heatmap\n",
    "metrics_data = []\n",
    "for i, stage in enumerate(sequential_results[\"stages\"]):\n",
    "    for lang in [\"english\", \"french\", \"italian\"]:\n",
    "        for metric in [\"precision\", \"recall\", \"f1\"]:\n",
    "            metrics_data.append({\n",
    "                \"Stage\": stage,\n",
    "                \"Test Set\": lang.capitalize(),\n",
    "                \"Metric\": metric.capitalize(),\n",
    "                \"Value\": sequential_results[f\"{lang}_test\"][metric][i]\n",
    "            })\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Create pivot tables for each metric\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, metric in enumerate([\"Precision\", \"Recall\", \"F1\"]):\n",
    "    pivot = df_metrics[df_metrics[\"Metric\"] == metric].pivot(\n",
    "        index=\"Test Set\", columns=\"Stage\", values=\"Value\"\n",
    "    )\n",
    "    # Reorder columns\n",
    "    pivot = pivot[sequential_results[\"stages\"]]\n",
    "    \n",
    "    sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"RdYlGn\", \n",
    "                vmin=0, vmax=1, ax=axes[idx], cbar_kws={'label': metric})\n",
    "    axes[idx].set_title(f'{metric} by Language and Training Stage', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('')\n",
    "    axes[idx].set_ylabel('')\n",
    "\n",
    "plt.suptitle('Sequential Training Performance Heatmap', fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / \"sequential_training_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved heatmap to {RESULTS_PATH / 'sequential_training_heatmap.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Sequential Training Approach:**\n",
    "- Train on English → Evaluate all test sets\n",
    "- Continue training on French → Evaluate all test sets\n",
    "- Continue training on Italian → Evaluate all test sets\n",
    "\n",
    "This approach reveals:\n",
    "1. **Zero-shot cross-lingual transfer**: How well does the model perform on French/Italian after training only on English?\n",
    "2. **Incremental learning**: Does adding each language's data improve or hurt performance on previously learned languages?\n",
    "3. **Catastrophic forgetting**: Does the model forget English after training on French and Italian?\n",
    "\n",
    "**Files created:**\n",
    "- `models/litbank-xlm-roberta-sequential/` - Final model (trained on EN → FR → IT)\n",
    "- `results/sequential_training_results.json` - All metrics from each training stage\n",
    "- `results/sequential_training_evolution.png` - Line plots showing metric evolution\n",
    "- `results/sequential_training_heatmap.png` - Heatmap comparison\n",
    "\n",
    "**Next steps:**\n",
    "Proceed to **Notebook 3: Multilingual Evaluation** for deeper analysis of the final model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
